<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>Krishna Teja Chitty-Venkata | Home</title>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Krishna Teja Chitty-Venkata</title>
<link rel="stylesheet" href="jemdoc.css" type="text/css" />

</head>

<body style="background-color:grey;" >
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">About Me</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<!--<div class="menu-item"><a href="home/contact.html">Contact</a></div>-->
<div class="menu-item"><a href="home/resume.html">Curriculum Vitae</a></div>

<div class="menu-category">Technical Expertise</div>
<div class="menu-item"><a href="technical/research.html">Publications</a></div>
<div class="menu-item"><a href="technical/internships.html">Internships</a></div>
<div class="menu-item"><a href="technical/projects.html">Projects</a></div>
<div class="menu-item"><a href="technical/teaching.html">Teaching</a></div>
<div class="menu-item"><a href="technical/blogs.html">Blogs</a></div>

<div class="menu-category">Photo Gallery</div>
<div class="menu-item"><a href="outside_class/oncampus.html">On Campus</a></div>
<div class="menu-item"><a href="outside_class/trips.html">Nature Trips</a></div>

</td>

<td id="layout-content">
<div id="toptitle">
<h1>Krishna Teja Chitty-Venkata</h1>
</div>

  <!--
  <table class="imgtable"><tr>
    
    
    <td>
<img align="center" src="images/asap_into.jpg" alt="Picture of Krishna Teja" class="center" width="280px" height="390px" />&nbsp;</td>
    
    <a href="http://www.iastate.edu/" target="_blank"><img src="images/isu_white.png" width="240" height="180" align="right" hspace="10px"></a>
           

    </tr></table>
-->
  
<table class="imgtable"><tr><td>
<img src="images/asap_into.jpg" alt="SC" width="216px" height="324px" />&nbsp;</td>
<td align="left"><p> Postdoctoral Researcher<br />
<a href="https://www.alcf.anl.gov/about/people/group/506" target="_blank">Data Science Group at Argonne Leadership Computing Facility</a> <br />
<a href="https://www.anl.gov/" target="_blank">Argonne National Laboratory</a> <br /><br />
  
 </p>
 


<p> Contact Information: <br />

Office: <a href="https://goo.gl/maps/6cXtkAuJG3bHmUuw9" target="_blank"> 9700 S Cass Ave, Lemont, IL</a><br />
Email: schittyvenkata AT anl DOT gov<br /><br />
<a href="https://www.linkedin.com/in/kt95/" target="_blank">LinkedIn</a><br />
<a href="https://github.com/krishnateja95" target="_blank">GitHub</a><br />
<a href="https://scholar.google.com/citations?user=KCU7b0MAAAAJ&hl=en" target="_blank">Google Scholar</a><br />
<a href="https://www.researchgate.net/profile/Krishna-Teja-Chitty-Venkata" target="_blank">ResearchGate</a><br />  
<a href="https://orcid.org/0000-0002-3027-1915" target="_blank">ORCID</a></p>

</td></tr></table>

 </br>
  

<h2>About Me</h2>
  
  <ul>
  
  <li> 
    <p style="font-size:115%;" align="justify">  
      I am a passionate and enthusiastic researcher, working primarily at the intersection of Systems and Deep Learning <br><br>  
     </p>  
  </li>

<li> 
    <p style="font-size:115%;" align="justify">  
     
      I am currently working as a Postdoctoral Researcher at Argonne National Laboratory 
      (US Department of Energy, Office of Science) with 
      <a href="https://memani1.github.io/" target="_blank">Murali Emani</a> and
      <a href="https://www.alcf.anl.gov/about/people/venkatram-vishwanath" target="_blank">Venkatram Vishwanath</a>  <br><br>  
     </p>  
  </li>



<li> 
    <p style="font-size:115%;" align="justify">  
      I finished my PhD from Iowa State University in Computer Engineering, under the guidance of
      <a href="http://class.ece.iastate.edu/arun/" target="_blank">Prof. Arun Somani</a> <br><br>  
     </p>  
  </li>



<li> 
    <p style="font-size:115%;" align="justify">  
     My research interests span across Machine Learning (ML), Deep Learning Inference Optimization, Computer Architecture,
      High Performance Computing (HPC), Computer Vision, AI Accelerators and Large Language Models (LLMs)  <br><br>  
     </p>  
  </li>    

    

    
 
</ul>


  
<h2>Education</h2>
  


<ul>
  
  <li> 
    <p style="font-size:115%;" align="justify">  
      <a href="http://www.iastate.edu/" target="_blank">Iowa State University, Ames, Iowa, USA </a>  <br>
      Doctor of Philosophy (PhD) <br>
      <a href="https://www.ece.iastate.edu/" target="_blank">Department of Electrical and Computer Engineering</a> <br>
      August 2017 - July 2023 <br>  
      
      Dissertation Title: Hardware-aware Design, Search and Optimization of Deep Neural Networks<br> 
      PhD Supervisor: Prof. <a href="http://class.ece.iastate.edu/arun/" target="_blank">Dr. Arun Somani</a> <br> <br>
      
     </p>  
  </li>
  
  
  <li> 
    <p style="font-size:115%;" align="justify">  
      University College of Engineering, Osmania University, Hyderabad, India <br> 
      Bachelor of Engineering <br>
      Electronics and Communication Engineering <br>
      2013 - 2017 <br>
  </p>
    
  </li>
  
</ul>  
  
  

  
  
<!--  
  
<p style="font-size:115%;" align="justify">I'm a Doctoral (PhD) student in the <a href="https://www.ece.iastate.edu/" target="_blank">Department of Electrical and Computer Engineering</a> at <a href="http://www.iastate.edu/" target="_blank">Iowa State University</a>, working under the supervision of <a href="http://class.ece.iastate.edu/arun/" target="_blank">Dr. Arun Somani</a> in <a href="http://dcnl.ece.iastate.edu/dcnl/" target="_blank">Dependable Computing and Networking Laboratory</a>. </br></br> I received my Bachelor's degree in Electronics and Communication Engineering from University College of Engineering, Osmania University, Hyderabad, India in Summer 2017.</p>
    
-->

  
<h2>PhD Research</h2>
  
  <ul>
  
  <li> 
    <p style="font-size:115%;" align="justify">  
      I worked as a Research Assistant under the supervision of <a href="http://class.ece.iastate.edu/arun/" target="_blank">Dr. Arun Somani</a> in the <a href="http://dcnl.ece.iastate.edu/dcnl/" target="_blank">Dependable Computing and Networking Laboratory</a> 
      on topics related to Deep Learning, Computer Architecture and Parallel Computing <br><br>  
     </p>  
  </li>
  
  
  <li> 
    <p style="font-size:115%;" align="justify">  
     I primarily worked at the intersection of Systems (Hardware Acceleration) and Deep Learning (Computer Vision) which include designing efficient
      Neural Network algorithms (Pruning, Quantization and Neural Architecture Search) for processing on Special Purpose
      Accelerators (TPU-like) and General Purpose devices (CPU, GPGPU). I also worked on different
      problems (fault tolerance, memory accesses) with respect to special purpose DNN hardware
  <br><br>
  </p>
  </li>
    
    
   <li> 
    <p style="font-size:115%;" align="justify">  
      The summary of my PhD research projects are as follows: <br><br>

<ol>
     
  <li>
     <big>
    We developed “Hardware Dimension Aware Pruning” (HDAP) method for array-based accelerators, multi-core CPUs, and Tensor Core GPUs by considering the underlying dimension
       of the system. In every pruning iteration, the HDAP algorithm prunes the number of nodes/filters in each layer based on the size of the underlying hardware 
       in consideration. We achieve an average speedup of 3.2x, whereas the baseline achieves only 1.5x on Turing Tensor Core-enabled GPU. 
       Our method attains an average speedup of 4.2x on the selected benchmarks on the Eyeriss hardware, whereas the baseline method attains only 1.6x.
       [<a href="https://ieeexplore.ieee.org/document/9153267" target="_blank">Paper 1</a>]
       [<a href="https://ieeexplore.ieee.org/abstract/document/9407921" target="_blank">Paper 2</a>] 
       
     </big>
  
  </li> </br>


   
  <li>
    <big>  We designed “Fault and Array size based Pruning” (FPAP) algorithm with the intent of bypassing the faults and removing the internal redundancy at the same time for 
    efficient inference. We compare our method with recent pruning methods under different fault scenarios and array sizes. We achieved a mean speedup of
    4.2x, where the baselines achieved 1.6x on ConvNet, NiN, AlexNet, and VGG16 over Eyeriss in the case of random faults. 
    [<a href="https://ieeexplore.ieee.org/abstract/document/9320358" target="_blank">Paper 1</a>]
    [<a href="https://ieeexplore.ieee.org/document/8825141" target="_blank">Paper 2</a>]
    </big>
     </li> </br>

  
<li>
   <big>  We developed “Mixed Sparse and Precision Search (MSPS) method to search for the optimal weight matrix type (sparse or dense) and precision combination
  for every layer of the pretrained model, such as ResNet50 which outperformed the manually designed Integer 8 ResNet50 network in terms of accuracy and
  latency. We extended the MSPS method and designed the “Architecture, Sparsity, and Precision Search” (ASPS) algorithm to find better model hyperparameters, matrix type,
  and precision in a single loop. The search method outperforms the manually tuned and automatically designed MSPS models on the ImageNet dataset. 
  The best ASPS model is 1.1x faster and 0.57% more accurate than the baseline sparse-only Integer 8 ResNet50 model [<a href="https://dl.acm.org/doi/10.1145/3502181.3531463" target="_blank">Paper</a>] 
   
   </big>
     </li> </br>

<li>
<big> We designed “Array Aware Neural Architecture Search” (AANAS) method to automatically design efficient CNNs for a 
  fixed array-based neural network accelerator. The previous Hardware-Aware Neural Architecture Search (HW-NAS) methods consider a fixed search
  space for different hardware platforms and search within its predefined space. We designed the search space based on the underlying hardware array dimensions
  to search for efficient CNN architectures for optimal inference performance.  [<a href="https://ieeexplore.ieee.org/document/9516612" target="_blank">Paper</a>] 
 </big>
     </li> </br>




  

<li>
    <big> We conducted an in-depth and state-of-the-art review of several Hardware-aware Neural Architecture Search (NAS) algorithms for MCU, CPU (mobile and desktop), GPU (Edge and server-level),
  ASIC, FPGA, ReRAM, DSP, and VPU platforms. We also reviewed hardware-aware mixed precision search algorithms and co-search methodologies of Neural algorithms and 
  accelerators. The paper guides researchers in choosing the best possible algorithm for a given application and hardware [<a href="https://dl.acm.org/doi/10.1145/3524500" target="_blank">Paper</a>] </big>

     </li> </br>
  

<li>
  <big>  We conducted an in-depth review of Neural Architecture Search techniques, targeting the Transformer model and its family of architectures, 
  such as Bidirectional Encoder Representations from Transformers (BERT) and Vision Transformers.Transformer-based Deep Neural Network architectures 
  have gained tremendous interest due to their effectiveness in various applications across Natural Language Processing (NLP) and Computer Vision (CV) domains [<a href="https://ieeexplore.ieee.org/document/9913476" target="_blank">Paper</a>] </big>
     </li> </br>




<li>
  <big>  We conducted an in-depth review of Neural Architecture Search Benchmarks (NAS-Bench) by  investigating the necessity, types and challenges in a NAS benchmark design. 
  A typical NAS benchmark consists of precomputed evaluation metrics, such as the validation accuracy, FLOPs, number of parameters, and latency on the hardware
  of all the neural architectures present in the predefined search space. We also provide future directions in this fast growing space and the result of
  our study in the growing field will assist the research community in developing a better next generation of NAS benchmarks [<a href="https://ieeexplore.ieee.org/document/10063950" target="_blank">Paper</a>] </big>
     </li> </br>
  



<li>
  <big>  We conducted a comprehensive survey of techniques for optimizing the inference phase of transformer models. We reviewed knowledge distillation, pruning, quantization,
  neural architecture search and lightweight network design techniques at the algorithmic level and hardware
  level optimization techniques for novel hardware accelerators for transformers. 
  [<a href="https://arxiv.org/abs/2307.07982" target="_blank">Arxiv Paper</a>]
  </big>
     </li> </br>


  
  
  
 </ol>   


      
  </p>
  </li>  
  
</ul>  
   
  
  
  
  
  
  
  </br>

</br>
</br>
</br>




   
    
     <img src="images/intro_slide.jpg" width="850" height="550" align="center" hspace="10px"> </br>

   <img src="images/Research_Big_Picture.png" width="1150" height="550" align="center" hspace="10px"> </br>


  
           
    
    
<div id="footer">
<div id="footer-text">
Page generated 2015-10-23 18:04:23 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
